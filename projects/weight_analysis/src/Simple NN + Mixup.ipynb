{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/jialin_song/anaconda3/envs/round11_explore/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchdiffeq\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def bootstrap_performance(X, y, clf, n=10, test_size=.2, eps=.01):\n",
    "    all_cross_entropy, all_accuracy = [], []\n",
    "    for i in tqdm(range(n)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i)\n",
    "        \n",
    "        clf.set_params(random_state=i)            \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        all_cross_entropy.append(log_loss(y_test, clf.predict_proba(X_test), eps=eps))\n",
    "        all_accuracy.append(clf.score(X_test, y_test))\n",
    "    return all_cross_entropy, all_accuracy\n",
    "\n",
    "\n",
    "#clf = GradientBoostingClassifier(learning_rate=0.015, n_estimators=300, max_depth=6, min_samples_split=30, min_samples_leaf=16)\n",
    "#X = np.load('./train_X_mobilenet_v2.npy')\n",
    "#y = np.load('./train_y_mobilenet_v2.npy')\n",
    "\n",
    "#cen, acc = bootstrap_performance(X, y, clf, n=20)\n",
    "#print(f'mean cross entropy: {np.mean(cen)}')\n",
    "#print(f'mean accuracy: {np.mean(acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    \"\"\"Set one seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sklearn to generate a random toy dataset\n",
    "#X, y = make_circles(n_samples=300, factor=0.6, noise=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/weight_stats/train_X_mobilenet_v2.npy')\n",
    "y = np.load('/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/weight_stats/train_y_mobilenet_v2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 948)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def get_lr(step, total_steps, lr_max, lr_min):\n",
    "  \"\"\"Compute learning rate according to cosine annealing schedule.\"\"\"\n",
    "  return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a torch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-2 #5e-3\n",
    "NUMBER_EPOCHS = 800\n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d8b7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 0.954047679901123\n",
      "Epoch: 199, Loss: 0.8746837377548218\n",
      "Epoch: 399, Loss: 0.4702509939670563\n",
      "Epoch: 599, Loss: 0.44249042868614197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:46<06:54, 46.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.4262056350708008\n",
      "0.3888330549001694\n",
      "Epoch: 50, Loss: 2.377702236175537\n",
      "Epoch: 199, Loss: 1.196948766708374\n",
      "Epoch: 399, Loss: 2.1547210216522217\n",
      "Epoch: 599, Loss: 0.5175000429153442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:36<06:29, 48.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.48510199785232544\n",
      "0.49462076723575593\n",
      "Epoch: 50, Loss: 0.7253899574279785\n",
      "Epoch: 199, Loss: 0.5926820635795593\n",
      "Epoch: 399, Loss: 0.6599323749542236\n",
      "Epoch: 599, Loss: 0.6928311586380005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:24<05:38, 48.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.6407779455184937\n",
      "0.6503372725099326\n",
      "Epoch: 50, Loss: 2.640005111694336\n",
      "Epoch: 199, Loss: 1.2846449613571167\n",
      "Epoch: 399, Loss: 0.7999675869941711\n",
      "Epoch: 599, Loss: 0.8757997751235962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:13<04:50, 48.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.7972866296768188\n",
      "0.5702416054904461\n",
      "Epoch: 50, Loss: 1.3878971338272095\n",
      "Epoch: 199, Loss: 1.2104965448379517\n",
      "Epoch: 399, Loss: 0.5020182132720947\n",
      "Epoch: 599, Loss: 0.4654081463813782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:58<03:57, 47.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.49648746848106384\n",
      "0.5025696609169245\n",
      "Epoch: 50, Loss: 0.758650004863739\n",
      "Epoch: 199, Loss: 0.68907630443573\n",
      "Epoch: 399, Loss: 0.599901020526886\n",
      "Epoch: 599, Loss: 0.6559334993362427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [04:42<03:05, 46.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.6424709558486938\n",
      "0.6227991584688425\n",
      "Epoch: 50, Loss: 0.7170708775520325\n",
      "Epoch: 199, Loss: 0.841819167137146\n",
      "Epoch: 399, Loss: 0.8811086416244507\n",
      "Epoch: 599, Loss: 0.5732848048210144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [05:25<02:15, 45.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.7113984227180481\n",
      "0.6708385396748782\n",
      "Epoch: 50, Loss: 4.652772903442383\n",
      "Epoch: 199, Loss: 0.5443645715713501\n",
      "Epoch: 399, Loss: 0.5127273797988892\n",
      "Epoch: 599, Loss: 0.4478883147239685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [06:09<01:29, 44.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.46080970764160156\n",
      "0.47851885482668877\n",
      "Epoch: 50, Loss: 1.4569380283355713\n",
      "Epoch: 199, Loss: 0.929425060749054\n",
      "Epoch: 399, Loss: 0.8255554437637329\n",
      "Epoch: 599, Loss: 0.7831293344497681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [06:57<00:45, 45.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.8108711242675781\n",
      "0.7995184458792209\n",
      "Epoch: 50, Loss: 5.258440971374512\n",
      "Epoch: 199, Loss: 0.6939379572868347\n",
      "Epoch: 399, Loss: 0.6022968292236328\n",
      "Epoch: 599, Loss: 0.6284703016281128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:45<00:00, 46.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 799, Loss: 0.58275306224823\n",
      "0.5921660248190165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "    BATCH_SIZE = 5 # define batch size\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float),\n",
    "        torch.tensor(y_train, dtype=torch.float).reshape(-1,1))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float),\n",
    "        torch.tensor(y_test, dtype=torch.float).reshape(-1,1))\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=500)\n",
    "\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(948, 128) # hidden layer\n",
    "            self.fc2 = nn.Linear(128, 32) # hidden layer\n",
    "            self.fc4 = nn.Linear(32, 1)   # ouput layer\n",
    "\n",
    "            self.drop = torch.nn.Dropout(p=0.15, inplace=False)\n",
    "        def forward(self, x):\n",
    "            x = self.drop(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc4(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    net = Net()\n",
    "    #print(net)   \n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss() # Sigmoid layer + Binary Cross Entropy\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: get_lr(  # pylint: disable=g-long-lambda\n",
    "            step, NUMBER_EPOCHS * len(train_loader),\n",
    "            1,  # lr_lambda computes multiplicative factor\n",
    "            1e-8 / LEARNING_RATE))\n",
    "\n",
    "    loss_hist = []\n",
    "    for epoch in range(1, NUMBER_EPOCHS):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha)\n",
    "            inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)        \n",
    "\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "\n",
    "            net.eval()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets) \n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        if epoch == 50: print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "        if (epoch+1) % 200 == 0: print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    #plt.plot(loss_hist)\n",
    "    #plt.ylabel('loss', fontsize=20)\n",
    "    #plt.xlabel('epoch', fontsize=20)       \n",
    "\n",
    "\n",
    "    net.eval()\n",
    "    preds = []\n",
    "    test_targets = []\n",
    "    sig = nn.Sigmoid()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        pred = net(inputs)\n",
    "        pred = sig(pred)\n",
    "        preds.append(pred.detach().numpy())\n",
    "        test_targets.append(targets.detach().numpy())\n",
    "\n",
    "\n",
    "    preds = np.asarray(preds).reshape(-1,1)\n",
    "    test_targets = np.asarray(test_targets).flatten()  \n",
    "\n",
    "    test_errors.append(log_loss(test_targets, preds, eps=.1))\n",
    "    print(test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae7a963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), '/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/mbnet_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e491c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors.append(log_loss(test_targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46254294823641734\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(948, 128) # hidden layer\n",
    "        self.fc2 = nn.Linear(128, 32) # hidden layer\n",
    "        self.fc4 = nn.Linear(32, 1)   # ouput layer\n",
    "\n",
    "        self.drop = torch.nn.Dropout(p=0.1, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "test_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net.load_state_dict(torch.load('/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/mbnet_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583745114534395\n"
     ]
    }
   ],
   "source": [
    "test_net.eval()\n",
    "preds = []\n",
    "test_targets = []\n",
    "sig = nn.Sigmoid()\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    pred = net(inputs)\n",
    "    pred = sig(pred)\n",
    "    preds.append(pred.detach().numpy())\n",
    "    test_targets.append(targets.detach().numpy())\n",
    "\n",
    "\n",
    "preds = np.asarray(preds).reshape(-1,1)\n",
    "test_targets = np.asarray(test_targets).flatten()  \n",
    "\n",
    "test_errors.append(log_loss(test_targets, preds))\n",
    "print(test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c52c9a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 948])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25f5d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85b6cf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4906],\n",
       "        [0.7241],\n",
       "        [0.4544],\n",
       "        [0.8758],\n",
       "        [0.1423],\n",
       "        [0.8272],\n",
       "        [0.2249],\n",
       "        [0.4057],\n",
       "        [0.5117],\n",
       "        [0.0467],\n",
       "        [0.4976],\n",
       "        [0.5859],\n",
       "        [0.1714],\n",
       "        [0.1997],\n",
       "        [0.0274],\n",
       "        [0.8140],\n",
       "        [0.6003],\n",
       "        [0.6562],\n",
       "        [0.9998],\n",
       "        [0.8612]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('round11_explore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b3d3baff363ffdd8f715871afbbb0158a35eb16105cf8b9c52841935e588fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
